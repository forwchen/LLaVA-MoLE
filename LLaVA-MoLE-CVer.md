---


---

<h1 id="llava-mole：用稀疏混合lora专家缓解指令微调mllm中的数据冲突">LLaVA-MoLE：用稀疏混合LoRA专家缓解指令微调MLLM中的数据冲突</h1>
<p><a href="https://imgse.com/i/pFMSB4A"><img src="https://s11.ax1x.com/2024/01/31/pFMSB4A.png" alt="pFMSB4A.png"></a></p>
<p>LLaVA-MoLE: Sparse Mixture of LoRA Experts for Mitigating Data Conflicts in Instruction Finetuning MLLMs</p>
<p>论文：<a href="https://arxiv.org/abs/2401.16160">https://arxiv.org/abs/2401.16160</a></p>
<h2 id="研究背景">研究背景</h2>
<p>近期，多模态大语言模型（MLLM）展现出广泛的图像内容理解和问答能力，这一方面得益于其以强大的大语言模型作为基座，另一方面，用丰富的图文指令数据进行指令微调（Instrcution Finetuning）也是十分关键的，其中除了指令数据本身的质量之外，不同类型的指令数据的配置也尤为重要。该研究发现在混合来自不同领域的指令数据时，数据冲突（Data Conflict）现象不可避免，导致训练后的模型在某一领域的性能相比使用单一领域数据时显著下降，这严重限制了通过增加训练数据类型来扩展模型能力范围。</p>
<p><a href="https://imgse.com/i/pFKbycQ"><img src="https://s11.ax1x.com/2024/01/31/pFKbycQ.png" alt="pFKbycQ.png"> 图1. 不同领域指令数据微调的模型在各领域Benchmark上的表现</a></p>
<p>如图1所示，在初步实验中，该研究在通用、文档、医疗等三个领域的指令数据上分别微调了一个LLaVA模型（LLaVA-1.5，LLaVA-Doc，以及LLaVA-Med），并在各领域Benchmark上进行测试。各个模型分别在对应训练数据的领域Benchmark上表现较好。但若简单地混合三个领域数据训练LLaVA-Mix模型，则会发现通用Benchmark上的性能有显著下降（306.3-&gt;295.8）。尽管其他两个领域的能力变化不大，甚至文档能力还会受益于通用指令数据，但整体来说，这样的结果证明当前模型架构下，简单混合多领域指令数据的可扩展性受限于数据间的冲突。而该研究提出的LLaVA-MoLE方法可以很好地在混合多领域数据时维持住各领域能力，甚至略有上升。</p>
<h2 id="方法">方法</h2>
<p>基于以上观察到的数据冲突现象，该研究提出了一种稀疏的混合LoRA专家（sparse Mixture of LoRA Experts，MoLE）架构，通过学习将token路由到合适的专家模块的方式，缓解不同领域的数据冲突。同时，该方法也是对常用的LoRA微调范式的高效扩展，以稀疏激活Top-1专家的方式，理论上相比于原始的LoRA仅增加了少量的路由计算开销，基本保持了总体计算量不增加。该研究的模型基于LLaVA-1.5的架构，因此命名为LLaVA-MoLE。</p>
<p><a href="https://imgse.com/i/pFKbR7q"><img src="https://s11.ax1x.com/2024/01/31/pFKbR7q.png" alt="pFKbR7q.png">图2. LLaVA-MoLE的模型结构</a></p>
<p>如图2所示，模型的视觉编码器、Adapter模块以及文本处理方式与LLaVA-1.5一致。当视觉和文本token拼接进入语言模型（Vicuna-1.5）之后，每一个Transformer层都经过了MoLE的改造。具体地，Self-Attention上的LoRA维持不变，仅在FFN层进行MoLE。对于每一个token，Router模块都会计算K个专家的一个概率分布，并取其中概率最高的专家激活，即进行实际的LoRA计算，其余专家跳过对该token的计算。最后，只有激活专家的数据与原本FFN的输出相加，得到该token在当前Transformer层的最终输出。</p>
<p>为了进行高效的实现，该研究在每一层首先计算所有token的专家分配，并依据专家分配将token分组（K个专家即分成K组），使得各个组内token的FFN和LoRA计算可以高效地并行进行，最后再按照原始token序列的顺序恢复位置。同时，为了保持各组token的计算量均衡，也即各个LoRA专家的token训练量均衡，该研究参考此前的MoE工作引入额外的负载均衡损失，控制各个专家上分配的token数量大致相当。最后，该研究的实现相比原始LoRA只有Router的额外计算量，在专家数量较少的情况下，总体计算量基本不增加。</p>
<h2 id="数据集">数据集</h2>
<p>与LLaVA-1.5相同，LLaVA-MoLE的模型经过两阶段训练，第一阶段使用了ShareGPT4V预训练数据，包含130万条图像及描述对，训练得到的一阶段预训练模型作为所有后续指令微调模型的基础。</p>
<p>指令数据方面，选取通用多任务问答，文档图表问答，以及医疗影像问答等三个领域的代表性数据集。M<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">^3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.814108em; vertical-align: 0em;"></span><span class="mord"><span class=""></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span></span>IT<sup class="footnote-ref"><a href="#fn1" id="fnref1">1</a></sup> 和ShareGPT4V-FT<sup class="footnote-ref"><a href="#fn2" id="fnref2">2</a></sup> 都是包含多个视觉多任务问答指令数据的高质量数据集，被用作通用多任务问答指令数据，总计230万条。文档图表问答则选取UReader<sup class="footnote-ref"><a href="#fn3" id="fnref3">3</a></sup>基于多个公开文档图表问答数据集构建的综合数据集，并采用UReader调节好的数据配比，共110万条指令数据。医疗影响方面选择PathVQA<sup class="footnote-ref"><a href="#fn4" id="fnref4">4</a></sup> 作为指令微调数据，为确保在该数据集上充分收敛，该研究参考LLaVA-Med<sup class="footnote-ref"><a href="#fn5" id="fnref5">5</a></sup>将数据集重复采样至40万条指令数据。</p>
<p>Benchmark方面，Tiny LVLM-eHub<sup class="footnote-ref"><a href="#fn6" id="fnref6">6</a></sup>是广泛采用的MLLM通用多任务基准测试集，其包含5大项共42个细分数据集的综合评测，可以充分反映模型的通用多任务问答能力。文档图表问答能力则用UReader中的ChartQA和DocQA子集进行评测，医疗影响方面也使用PathVQA的测试集，评测开放和固定选项问答两类。</p>
<h2 id="实验">实验</h2>
<p><a href="https://imgse.com/i/pFKb2Bn"><img src="https://s11.ax1x.com/2024/01/31/pFKb2Bn.png" alt="pFKb2Bn.png">图3. 不同模型在各种数据配置训练后的评测结果</a></p>
<p>如图3所示，该研究对不同模型在各种数据配置下进行充分的实验。复现的LLaVA-1.5<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mo>†</mo></msup></mrow><annotation encoding="application/x-tex">^\dagger</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.849108em; vertical-align: 0em;"></span><span class="mord"><span class=""></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.849108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">†</span></span></span></span></span></span></span></span></span></span></span></span>，LLaVA-Doc<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mo>†</mo></msup></mrow><annotation encoding="application/x-tex">^\dagger</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.849108em; vertical-align: 0em;"></span><span class="mord"><span class=""></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.849108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">†</span></span></span></span></span></span></span></span></span></span></span></span>，LLaVA-Med<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mo>†</mo></msup></mrow><annotation encoding="application/x-tex">^\dagger</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.849108em; vertical-align: 0em;"></span><span class="mord"><span class=""></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.849108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">†</span></span></span></span></span></span></span></span></span></span></span></span>分别在通用多任务、文档图表、医疗影响的测试上取得了较好的性能，基本相当于现有公开模型的性能。在进行数据混合之后，可以发现LLaVA-Mix模型的通用多任务问答性能容易受到其他领域数据加入的干扰，从而在不同的混合配置下有不同程度的显著下降（综合分数下降7.0到9.1）。尤其对第9、10组实验来说，增加文档图表指令数据可以有效地提升相应的问答能力，但通用多任务问答性能会进一步下降5.1到8.6，<strong>这会对数据层面的扩展造成比较大的障碍</strong>。对比相同数据配置的LLaVA-MoLE和LLaVA-Mix模型，可以发现数据冲突现象被LLaVA-MoLE很好地缓解了，通用问答性能基本持平LLaVA-1.5的同时，文档图表和医疗影响方面的性能甚至可以进一步受益，均超越了单一领域微调的模型。</p>
<p><a href="https://imgse.com/i/pFKb6Xj"><img src="https://s11.ax1x.com/2024/01/31/pFKb6Xj.png" alt="pFKb6Xj.png">图4. LoRA Rank的影响</a></p>
<p>在图4的实验中，调节LLaVA-Mix模型的LoRA rank，该研究发现在32-96的rank时，模型都会受到数据冲突的影响，即混合两个领域数据的模型都出现了通用多任务问答性能下降的现象。当LoRA rank提升到128时，模型容量显著增大，此时模型看起来缓解了数据冲突，但代价是提升4倍的额外参数量。相比之下，LLaVA-MoLE在较小的LoRA rank下就能够缓解数据冲突，无论在参数量和计算量上都更具优势。并且，在LoRA rank提升到128后，LLaVA-MoLE在缓解数据冲突的基础上能够实现更大幅度的性能提升，尤其在ChartQA上相比LLaVA-Mix提升了11.3%。</p>
<p><a href="https://imgse.com/i/pFKbs1g"><img src="https://s11.ax1x.com/2024/01/31/pFKbs1g.png" alt="pFKbs1g.png">图5. LoRA专家数量的影响</a></p>
<p>图5展示了不同LoRA专家数量对性能的影响，尽管使用2~3个专家就可以较好地缓解数据冲突，该研究发现继续增加专家数量可以带来更大的提升。虽然模型实际激活的参数量和计算量并没有变化，但潜在参数量的提升可以帮助模型有更大的余地去进行更细致的专家分配，前提是各个专家能够受到充分地训练。因此当专家数据继续提高之后，由于分配到每个专家的token数量下降，整体的性能也会回落。综合来说，专家数量的选择需要综合混合的领域数量和整体数据量进行考虑。</p>
<p><a href="https://imgse.com/i/pFKbgns"><img src="https://s11.ax1x.com/2024/01/31/pFKbgns.png" alt="pFKbgns.png">图6. 部分LLM层中的专家激活分布</a></p>
<p>最后，该研究在三个领域数据上分别可视化了部分LLM层中的专家激活分布。可以看到在某些层中（如第0、2层），各个专家的激活情况并不随领域变化，这些层对不同类型的token都按照类似的模式激活专家，但层与层之间还是存在专家激活分布的显著差异。在第10和28层中，都可以观察到不同领域的专家激活分布大不相同，并且综合所有层来看，0号专家略偏好于医疗影像数据。总的来说，由于负载均衡损失的约束，专家激活分布不会表现出对某一特定领域的显著偏好，这与一些现有MoE工作<sup class="footnote-ref"><a href="#fn7" id="fnref7">7</a></sup>中观察到的现象类似。</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>Li, Lei, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li et al. “M <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">^ 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.814108em; vertical-align: 0em;"></span><span class="mord"><span class=""></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span></span> IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning.” <em>arXiv preprint arXiv:2306.04387</em> (2023). <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p>Chen, Lin, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. “Sharegpt4v: Improving large multi-modal models with better captions.” <em>arXiv preprint arXiv:2311.12793</em> (2023). <a href="#fnref2" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn3" class="footnote-item"><p>Ye, Jiabo, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Guohai Xu, Chenliang Li et al. “Ureader: Universal ocr-free visually-situated language understanding with multimodal large language model.” <em>arXiv preprint arXiv:2310.05126</em> (2023). <a href="#fnref3" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn4" class="footnote-item"><p>He, Xuehai, Zhuo Cai, Wenlan Wei, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. “Pathological visual question answering.” <em>arXiv preprint arXiv:2010.12435</em> (2020). <a href="#fnref4" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn5" class="footnote-item"><p>Li, Chunyuan, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. “Llava-med: Training a large language-and-vision assistant for biomedicine in one day.” <em>arXiv preprint arXiv:2306.00890</em> (2023). <a href="#fnref5" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn6" class="footnote-item"><p>Shao, Wenqi, Yutao Hu, Peng Gao, Meng Lei, Kaipeng Zhang, Fanqing Meng, Peng Xu et al. “Tiny lvlm-ehub: Early multimodal experiments with bard.” <em>arXiv preprint arXiv:2308.03729</em> (2023). <a href="#fnref6" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn7" class="footnote-item"><p>Jiang, Albert Q., Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot et al. “Mixtral of experts.” <em>arXiv preprint arXiv:2401.04088</em> (2024). <a href="#fnref7" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>

